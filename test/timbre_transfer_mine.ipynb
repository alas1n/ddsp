{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title #Install and Import\n",
    "\n",
    "#@markdown Install ddsp, define some helper functions, and download the model. This transfers a lot of data and _should take a minute or two_.\n",
    "# %tensorflow_version 2.x\n",
    "print('Installing from pip package...')\n",
    "!pip install -qU ddsp==0.14.0\n",
    "\n",
    "# Ignore a bunch of deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import crepe\n",
    "import ddsp\n",
    "import ddsp.training\n",
    "# from ddsp.colab import colab_utils\n",
    "# from ddsp.colab.colab_utils import (\n",
    "#     auto_tune, detect_notes, fit_quantile_transform, \n",
    "#     get_tuning_factor, download, play, record, \n",
    "#     specplot, upload, DEFAULT_SAMPLE_RATE)\n",
    "import gin\n",
    "# from google.colab import files\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# IMPORTS MINE:\n",
    "import sounddevice as sd\n",
    "import time as stime\n",
    "import wavio\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_SAMPLE_RATE = 16000 # general sound frequecy\n",
    "sample_rate = DEFAULT_SAMPLE_RATE  # 16000 \n",
    "sd.default.samplerate  = sample_rate # defult samplerate for sounddevice library\n",
    "sd.default.channels = 1\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "DEFAULT_SAMPLE_RATE  = 16000\n",
    "sample_rate = DEFAULT_SAMPLE_RATE  # 16000\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### my Functions:\n",
    "def playsound(inputsound):\n",
    "    if len(inputsound) == 1:\n",
    "        sd.play(inputsound[0], DEFAULT_SAMPLE_RATE)\n",
    "        stime.sleep(len(inputsound[0])/DEFAULT_SAMPLE_RATE)\n",
    "        sd.stop()\n",
    "    else:\n",
    "        sd.play(inputsound, DEFAULT_SAMPLE_RATE)\n",
    "        stime.sleep(len(inputsound)/DEFAULT_SAMPLE_RATE)\n",
    "        sd.stop()\n",
    "\n",
    "\n",
    "def Myspecplot(inputsound):\n",
    "    if len(inputsound)==1:\n",
    "        plot = plt.figure(figsize=(4, 4))\n",
    "        powerSpectrum, freqenciesFound, time, imageAxis = plt.specgram(inputsound[0], Fs=DEFAULT_SAMPLE_RATE)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Frequency')\n",
    "        return plot   \n",
    "    else:\n",
    "        plot = plt.figure(figsize=(4, 4))\n",
    "        powerSpectrum, freqenciesFound, time, imageAxis = plt.specgram(inputsound, Fs=DEFAULT_SAMPLE_RATE)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Frequency')\n",
    "        return plot\n",
    "\n",
    "    \n",
    "# record sound and returen one arrry object\n",
    "def recorder(duration=5):\n",
    "    #duration = 5  # seconds\n",
    "    # defult sample rate is 1600\n",
    "    print('recording')\n",
    "    rec_audio = sd.rec(int(duration * sd.default.samplerate))\n",
    "    sd.wait()\n",
    "    print('end recording')\n",
    "    audio = []\n",
    "    for i in rec_audio:\n",
    "        audio.append(i[0])\n",
    "    audio = np.array(audio) \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other Functinos\n",
    "LD_RANGE = 120.0  # dB\n",
    "import librosa\n",
    "\n",
    "def pad_or_trim_to_expected_length(vector,\n",
    "                                   expected_len,\n",
    "                                   pad_value=0,\n",
    "                                   len_tolerance=20,\n",
    "                                   use_tf=False):\n",
    "  \"\"\"Make vector equal to the expected length.\n",
    "\n",
    "  Feature extraction functions like `compute_loudness()` or `compute_f0` produce\n",
    "  feature vectors that vary in length depending on factors such as `sample_rate`\n",
    "  or `hop_size`. This function corrects vectors to the expected length, warning\n",
    "  the user if the difference between the vector and expected length was\n",
    "  unusually high to begin with.\n",
    "\n",
    "  Args:\n",
    "    vector: Numpy 1D ndarray. Shape [vector_length,]\n",
    "    expected_len: Expected length of vector.\n",
    "    pad_value: Value to pad at end of vector.\n",
    "    len_tolerance: Tolerance of difference between original and desired vector\n",
    "      length.\n",
    "    use_tf: Make function differentiable by using tensorflow.\n",
    "\n",
    "  Returns:\n",
    "    vector: Vector with corrected length.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if `len(vector)` is different from `expected_len` beyond\n",
    "    `len_tolerance` to begin with.\n",
    "  \"\"\"\n",
    "  expected_len = int(expected_len)\n",
    "  vector_len = int(vector.shape[-1])\n",
    "\n",
    "  if abs(vector_len - expected_len) > len_tolerance:\n",
    "    # Ensure vector was close to expected length to begin with\n",
    "    raise ValueError('Vector length: {} differs from expected length: {} '\n",
    "                     'beyond tolerance of : {}'.format(vector_len,\n",
    "                                                       expected_len,\n",
    "                                                       len_tolerance))\n",
    "  # Pick tensorflow or numpy.\n",
    "  lib = tf if use_tf else np\n",
    "\n",
    "  is_1d = (len(vector.shape) == 1)\n",
    "  vector = vector[lib.newaxis, :] if is_1d else vector\n",
    "\n",
    "  # Pad missing samples\n",
    "  if vector_len < expected_len:\n",
    "    n_padding = expected_len - vector_len\n",
    "    vector = lib.pad(\n",
    "        vector, ((0, 0), (0, n_padding)),\n",
    "        mode='constant',\n",
    "        constant_values=pad_value)\n",
    "  # Trim samples\n",
    "  elif vector_len > expected_len:\n",
    "    vector = vector[..., :expected_len]\n",
    "\n",
    "  # Remove temporary batch dimension.\n",
    "  vector = vector[0] if is_1d else vector\n",
    "  return vector\n",
    "\n",
    "\n",
    "def amplitude_to_db(amplitude, use_tf=False):\n",
    "  \"\"\"Converts amplitude to decibels.\"\"\"\n",
    "  lib = tf if use_tf else np\n",
    "  log10 = (lambda x: tf.math.log(x) / tf.math.log(10.0)) if use_tf else np.log10\n",
    "  amin = 1e-20  # Avoid log(0) instabilities.\n",
    "  db = log10(lib.maximum(amin, amplitude))\n",
    "  db *= 20.0\n",
    "  return db\n",
    "\n",
    "\n",
    "def stft_np(audio, frame_size=2048, overlap=0.75, pad_end=True):\n",
    "  \"\"\"Non-differentiable stft using librosa, one example at a time.\"\"\"\n",
    "  assert frame_size * overlap % 2.0 == 0.0\n",
    "  hop_size = int(frame_size * (1.0 - overlap))\n",
    "  is_2d = (len(audio.shape) == 2)\n",
    "\n",
    "  if pad_end:\n",
    "    n_samples_initial = int(audio.shape[-1])\n",
    "    n_frames = int(np.ceil(n_samples_initial / hop_size))\n",
    "    n_samples_final = (n_frames - 1) * hop_size + frame_size\n",
    "    pad = n_samples_final - n_samples_initial\n",
    "    padding = ((0, 0), (0, pad)) if is_2d else ((0, pad),)\n",
    "    audio = np.pad(audio, padding, 'constant')\n",
    "\n",
    "  def stft_fn(y):\n",
    "    return librosa.stft(y=y,\n",
    "                        n_fft=int(frame_size),\n",
    "                        hop_length=hop_size,\n",
    "                        center=False).T\n",
    "\n",
    "  s = np.stack([stft_fn(a) for a in audio]) if is_2d else stft_fn(audio)\n",
    "  return s\n",
    "\n",
    "\n",
    "\n",
    "def compute_loudness(audio,\n",
    "                     sample_rate=16000,\n",
    "                     frame_rate=250,\n",
    "                     n_fft=2048,\n",
    "                     range_db=LD_RANGE,\n",
    "                     ref_db=20.7,\n",
    "                     use_tf=False):\n",
    "  \"\"\"Perceptual loudness in dB, relative to white noise, amplitude=1.\n",
    "\n",
    "  Function is differentiable if use_tf=True.\n",
    "  Args:\n",
    "    audio: Numpy ndarray or tensor. Shape [batch_size, audio_length] or\n",
    "      [batch_size,].\n",
    "    sample_rate: Audio sample rate in Hz.\n",
    "    frame_rate: Rate of loudness frames in Hz.\n",
    "    n_fft: Fft window size.\n",
    "    range_db: Sets the dynamic range of loudness in decibles. The minimum\n",
    "      loudness (per a frequency bin) corresponds to -range_db.\n",
    "    ref_db: Sets the reference maximum perceptual loudness as given by\n",
    "      (A_weighting + 10 * log10(abs(stft(audio))**2.0). The default value\n",
    "      corresponds to white noise with amplitude=1.0 and n_fft=2048. There is a\n",
    "      slight dependence on fft_size due to different granularity of perceptual\n",
    "      weighting.\n",
    "    use_tf: Make function differentiable by using tensorflow.\n",
    "\n",
    "  Returns:\n",
    "    Loudness in decibels. Shape [batch_size, n_frames] or [n_frames,].\n",
    "  \"\"\"\n",
    "  if sample_rate % frame_rate != 0:\n",
    "    raise ValueError(\n",
    "        'frame_rate: {} must evenly divide sample_rate: {}.'\n",
    "        'For default frame_rate: 250Hz, suggested sample_rate: 16kHz or 48kHz'\n",
    "        .format(frame_rate, sample_rate))\n",
    "\n",
    "  # Pick tensorflow or numpy.\n",
    "  lib = tf if use_tf else np\n",
    "\n",
    "  # Make inputs tensors for tensorflow.\n",
    "  audio = tf_float32(audio) if use_tf else audio\n",
    "\n",
    "  # Temporarily a batch dimension for single examples.\n",
    "  is_1d = (len(audio.shape) == 1)\n",
    "  audio = audio[lib.newaxis, :] if is_1d else audio\n",
    "\n",
    "  # Take STFT.\n",
    "  hop_size = sample_rate // frame_rate\n",
    "  overlap = 1 - hop_size / n_fft\n",
    "  stft_fn = stft if use_tf else stft_np\n",
    "  s = stft_fn(audio, frame_size=n_fft, overlap=overlap, pad_end=True)\n",
    "\n",
    "  # Compute power.\n",
    "  amplitude = lib.abs(s)\n",
    "  power_db = amplitude_to_db(amplitude, use_tf=use_tf)\n",
    "\n",
    "  # Perceptual weighting.\n",
    "  frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=n_fft)\n",
    "  a_weighting = librosa.A_weighting(frequencies)[lib.newaxis, lib.newaxis, :]\n",
    "  loudness = power_db + a_weighting\n",
    "\n",
    "  # Set dynamic range.\n",
    "  loudness -= ref_db\n",
    "  loudness = lib.maximum(loudness, -range_db)\n",
    "  mean = tf.reduce_mean if use_tf else np.mean\n",
    "\n",
    "  # Average over frequency bins.\n",
    "  loudness = mean(loudness, axis=-1)\n",
    "\n",
    "  # Remove temporary batch dimension.\n",
    "  loudness = loudness[0] if is_1d else loudness\n",
    "\n",
    "  # Compute expected length of loudness vector\n",
    "  n_secs = audio.shape[-1] / float(\n",
    "      sample_rate)  # `n_secs` can have milliseconds\n",
    "  expected_len = int(n_secs * frame_rate)\n",
    "\n",
    "  # Pad with `-range_db` noise floor or trim vector\n",
    "  loudness = pad_or_trim_to_expected_length(\n",
    "      loudness, expected_len, -range_db, use_tf=use_tf)\n",
    "  return loudness\n",
    "\n",
    "\n",
    "def squeeze(input_vector):\n",
    "  \"\"\"Ensure vector only has one axis of dimensionality.\"\"\"\n",
    "  if input_vector.ndim > 1:\n",
    "    return np.squeeze(input_vector)\n",
    "  else:\n",
    "    return input_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crepe\n",
    "def compute_f0(audio, sample_rate, frame_rate, viterbi=True):\n",
    "  \"\"\"Fundamental frequency (f0) estimate using CREPE.\n",
    "\n",
    "  This function is non-differentiable and takes input as a numpy array.\n",
    "  Args:\n",
    "    audio: Numpy ndarray of single audio example. Shape [audio_length,].\n",
    "    sample_rate: Sample rate in Hz.\n",
    "    frame_rate: Rate of f0 frames in Hz.\n",
    "    viterbi: Use Viterbi decoding to estimate f0.\n",
    "\n",
    "  Returns:\n",
    "    f0_hz: Fundamental frequency in Hz. Shape [n_frames,].\n",
    "    f0_confidence: Confidence in Hz estimate (scaled [0, 1]). Shape [n_frames,].\n",
    "  \"\"\"\n",
    "\n",
    "  n_secs = len(audio) / float(sample_rate)  # `n_secs` can have milliseconds\n",
    "  crepe_step_size = 1000 / frame_rate  # milliseconds\n",
    "  expected_len = int(n_secs * frame_rate)\n",
    "  audio = np.asarray(audio)\n",
    "\n",
    "  # Compute f0 with crepe.\n",
    "  _, f0_hz, f0_confidence, _ = crepe.predict(\n",
    "      audio,\n",
    "      model_capacity='tiny',\n",
    "      sr=sample_rate,\n",
    "      viterbi=viterbi,\n",
    "      step_size=crepe_step_size,\n",
    "      center=False,\n",
    "      verbose=0)\n",
    "\n",
    "  # Postprocessing on f0_hz\n",
    "  f0_hz = pad_or_trim_to_expected_length(f0_hz, expected_len, 0)  # pad with 0\n",
    "  f0_hz = f0_hz.astype(np.float32)\n",
    "\n",
    "  # Postprocessing on f0_confidence\n",
    "  f0_confidence = pad_or_trim_to_expected_length(f0_confidence, expected_len, 1)\n",
    "  f0_confidence = np.nan_to_num(f0_confidence)   # Set nans to 0 in confidence\n",
    "  f0_confidence = f0_confidence.astype(np.float32)\n",
    "  return f0_hz, f0_confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Record or Upload Audio\n",
    "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
    "#@markdown * Audio should be monophonic (single instrument / voice)\n",
    "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
    "\n",
    "record_or_upload = \"Record\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
    "\n",
    "record_seconds =     5#@param {type:\"number\", min:1, max:10, step:1}\n",
    "\n",
    "if record_or_upload == \"Record\":\n",
    "    audio = recorder(8)\n",
    "# else:\n",
    "#   # Load audio sample here (.mp3 or .wav3 file)\n",
    "#   # Just use the first file.\n",
    "#   filenames, audios = upload()\n",
    "#   audio = audios[0]\n",
    "audio = audio[np.newaxis, :]\n",
    "# Plot.\n",
    "Myspecplot(audio)\n",
    "playsound(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nExtracting audio features...')\n",
    "n_fft=2048\n",
    "sample_rate=16000\n",
    "frame_rate=250\n",
    "\n",
    "# Setup the session.\n",
    "ddsp.spectral_ops.reset_crepe()\n",
    "\n",
    "# Compute features.\n",
    "start_time = time.time()\n",
    "audio_feats = {'audio': audio}\n",
    "audio = squeeze(audio)\n",
    "audio_feats['loudness_db'] = compute_loudness(audio, sample_rate, frame_rate, n_fft)\n",
    "audio_feats['f0_hz'], audio_feats['f0_confidence'] = (compute_f0(audio, sample_rate, frame_rate))\n",
    "\n",
    "audio_features = audio_feats\n",
    "# audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
    "audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
    "audio_features_mod = None\n",
    "print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRIM = -15\n",
    "# Plot Features.\n",
    "fig, ax = plt.subplots(nrows=3, \n",
    "                       ncols=1, \n",
    "                       sharex=True,\n",
    "                       figsize=(6, 8))\n",
    "ax[0].plot(audio_features['loudness_db'][:TRIM])\n",
    "ax[0].set_ylabel('loudness_db')\n",
    "\n",
    "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
    "ax[1].set_ylabel('f0 [midi]')\n",
    "\n",
    "ax[2].plot(audio_features['f0_confidence'][:TRIM])\n",
    "ax[2].set_ylabel('f0 confidence')\n",
    "_ = ax[2].set_xlabel('Time step [frame]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load a model\n",
    "#@markdown Run for ever new audio input\n",
    "model = 'Flute' #@param ['Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone', 'Upload your own (checkpoint folder as .zip)']\n",
    "selected_model = 'Trumpet'\n",
    "\n",
    "Models=[\n",
    "    {\n",
    "        \"Name\":\"Violin\",\n",
    "        \"gin_file\":\"content/pretrained/Violin/operative_config-0.gin\",\n",
    "        \"model_dir\":\"content/pretrained/Violin\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\":\"Flute\",\n",
    "        \"gin_file\":\"content/pretrained/Flute/operative_config-0.gin\",\n",
    "        \"model_dir\":\"content/pretrained/Flute\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\":\"Flute2\",\n",
    "        \"gin_file\":\"content/pretrained/Flute2/operative_config-0.gin\",\n",
    "        \"model_dir\":\"content/pretrained/Flute2\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\":\"Trumpet\",\n",
    "        \"gin_file\":\"content/pretrained/Trumpet/operative_config-0.gin\",\n",
    "        \"model_dir\":\"content/pretrained/Trumpet\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\":\"Tenor_Saxophone\",\n",
    "        \"gin_file\":\"content/pretrained/Tenor_Saxophone/operative_config-0.gin\",\n",
    "        \"model_dir\":\"content/pretrained/Tenor_Saxophone\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "gin_file=''\n",
    "model_dir=''\n",
    "\n",
    "for model in Models:\n",
    "    if model['Name'] == selected_model:\n",
    "        gin_file = model['gin_file']\n",
    "        model_dir = model['model_dir']\n",
    "\n",
    "# gin_file = 'content/pretrained/operative_config-0.gin'  \n",
    "# model_dir = 'content/pretrained'\n",
    "\n",
    "# Load the dataset statistics.\n",
    "DATASET_STATS = None\n",
    "dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n",
    "print(f'Loading dataset statistics from {dataset_stats_file}')\n",
    "try:\n",
    "  if tf.io.gfile.exists(dataset_stats_file):\n",
    "    with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
    "      DATASET_STATS = pickle.load(f)\n",
    "except Exception as err:\n",
    "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
    "\n",
    "\n",
    "# Parse gin config,\n",
    "with gin.unlock_config():\n",
    "  gin.parse_config_file(gin_file, skip_unknown=True)\n",
    "\n",
    "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
    "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
    "ckpt_name = ckpt_files[0].split('.')[0]\n",
    "ckpt = os.path.join(model_dir, ckpt_name)\n",
    "\n",
    "# Ensure dimensions and sampling rates are equal\n",
    "time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
    "n_samples_train = gin.query_parameter('Additive.n_samples')\n",
    "hop_size = int(n_samples_train / time_steps_train)\n",
    "\n",
    "time_steps = int(audio.shape[0] / hop_size)\n",
    "n_samples = time_steps * hop_size\n",
    "\n",
    "# print(\"===Trained model===\")\n",
    "# print(\"Time Steps\", time_steps_train)\n",
    "# print(\"Samples\", n_samples_train)\n",
    "# print(\"Hop Size\", hop_size)\n",
    "# print(\"\\n===Resynthesis===\")\n",
    "# print(\"Time Steps\", time_steps)\n",
    "# print(\"Samples\", n_samples)\n",
    "# print('')\n",
    "\n",
    "gin_params = [\n",
    "    'Additive.n_samples = {}'.format(n_samples),\n",
    "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
    "    'DefaultPreprocessor.time_steps = {}'.format(time_steps),\n",
    "    'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
    "]\n",
    "\n",
    "with gin.unlock_config():\n",
    "  gin.parse_config(gin_params)\n",
    "\n",
    "\n",
    "# Trim all input vectors to correct lengths \n",
    "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
    "  audio_features[key] = audio_features[key][:time_steps]\n",
    "# audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
    "audio_features['audio'] = audio_features['audio'][0]\n",
    "\n",
    "\n",
    "# Set up the model just to predict audio given new conditioning\n",
    "model = ddsp.training.models.Autoencoder()\n",
    "model.restore(ckpt)\n",
    "\n",
    "# Build model by running a batch through it.\n",
    "start_time = time.time()\n",
    "_ = model(audio_features, training=False)\n",
    "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Modify conditioning\n",
    "\n",
    "#@markdown These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true). \n",
    "\n",
    "\n",
    "#@markdown ## Note Detection\n",
    "\n",
    "#@markdown You can leave this at 1.0 for most cases\n",
    "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
    "\n",
    "\n",
    "#@markdown ## Automatic\n",
    "\n",
    "ADJUST = True #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown Quiet parts without notes detected (dB)\n",
    "quiet = 20 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
    "\n",
    "#@markdown Force pitch to nearest note (amount)\n",
    "autotune = 0 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
    "\n",
    "#@markdown ## Manual\n",
    "\n",
    "\n",
    "#@markdown Shift the pitch (octaves)\n",
    "pitch_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
    "\n",
    "#@markdown Adjsut the overall loudness (dB)\n",
    "loudness_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
    "\n",
    "\n",
    "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
    "\n",
    "\n",
    "## Helper functions.\n",
    "def shift_ld(audio_features, ld_shift=0.0):\n",
    "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
    "  audio_features['loudness_db'] += ld_shift\n",
    "  return audio_features\n",
    "\n",
    "\n",
    "def shift_f0(audio_features, pitch_shift=0.0):\n",
    "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
    "  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
    "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
    "                                    0.0, \n",
    "                                    librosa.midi_to_hz(110.0))\n",
    "  return audio_features\n",
    "\n",
    "\n",
    "mask_on = None\n",
    "\n",
    "if ADJUST and DATASET_STATS is not None:\n",
    "  # Detect sections that are \"on\".\n",
    "  mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n",
    "                                        audio_features['f0_confidence'],\n",
    "                                        threshold)\n",
    "\n",
    "  if np.any(mask_on):\n",
    "    # Shift the pitch register.\n",
    "    target_mean_pitch = DATASET_STATS['mean_pitch']\n",
    "    pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
    "    mean_pitch = np.mean(pitch[mask_on])\n",
    "    p_diff = target_mean_pitch - mean_pitch\n",
    "    p_diff_octave = p_diff / 12.0\n",
    "    round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
    "    p_diff_octave = round_fn(p_diff_octave)\n",
    "    audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
    "\n",
    "\n",
    "    # Quantile shift the note_on parts.\n",
    "    _, loudness_norm = colab_utils.fit_quantile_transform(\n",
    "        audio_features['loudness_db'],\n",
    "        mask_on,\n",
    "        inv_quantile=DATASET_STATS['quantile_transform'])\n",
    "\n",
    "    # Turn down the note_off parts.\n",
    "    mask_off = np.logical_not(mask_on)\n",
    "    loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n",
    "    loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n",
    "    \n",
    "    audio_features_mod['loudness_db'] = loudness_norm \n",
    "\n",
    "    # Auto-tune.\n",
    "    if autotune:\n",
    "      f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
    "      tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
    "      f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)\n",
    "      audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
    "\n",
    "  else:\n",
    "    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n",
    "\n",
    "else:\n",
    "  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n",
    "\n",
    "# Manual Shifts.\n",
    "audio_features_mod = shift_ld(audio_features_mod, loudness_shift)\n",
    "audio_features_mod = shift_f0(audio_features_mod, pitch_shift)\n",
    "\n",
    "\n",
    "\n",
    "# Plot Features.\n",
    "has_mask = int(mask_on is not None)\n",
    "n_plots = 3 if has_mask else 2 \n",
    "fig, axes = plt.subplots(nrows=n_plots, \n",
    "                      ncols=1, \n",
    "                      sharex=True,\n",
    "                      figsize=(2*n_plots, 8))\n",
    "\n",
    "if has_mask:\n",
    "  ax = axes[0]\n",
    "  ax.plot(np.ones_like(mask_on[:TRIM]) * threshold, 'k:')\n",
    "  ax.plot(note_on_value[:TRIM])\n",
    "  ax.plot(mask_on[:TRIM])\n",
    "  ax.set_ylabel('Note-on Mask')\n",
    "  ax.set_xlabel('Time step [frame]')\n",
    "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
    "\n",
    "ax = axes[0 + has_mask]\n",
    "ax.plot(audio_features['loudness_db'][:TRIM])\n",
    "ax.plot(audio_features_mod['loudness_db'][:TRIM])\n",
    "ax.set_ylabel('loudness_db')\n",
    "ax.legend(['Original','Adjusted'])\n",
    "\n",
    "ax = axes[1 + has_mask]\n",
    "ax.plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
    "ax.plot(librosa.hz_to_midi(audio_features_mod['f0_hz'][:TRIM]))\n",
    "ax.set_ylabel('f0 [midi]')\n",
    "_ = ax.legend(['Original','Adjusted'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title #Resynthesize Audio\n",
    "\n",
    "af = audio_features if audio_features_mod is None else audio_features_mod\n",
    "\n",
    "# Run a batch of predictions.\n",
    "start_time = time.time()\n",
    "outputs = model(af, training=False)\n",
    "audio_gen = model.get_audio_from_outputs(outputs)\n",
    "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
    "\n",
    "# Plot\n",
    "print('Original')\n",
    "playsound(audio)\n",
    "\n",
    "print('Resynthesis')\n",
    "playsound(audio_gen)\n",
    "\n",
    "Myspecplot(audio)\n",
    "plt.title(\"Original\")\n",
    "\n",
    "Myspecplot(audio_gen)\n",
    "_ = plt.title(\"Resynthesis\")\n",
    "\n",
    "wavio.write(\"Original.wav\", audio, sample_rate, sampwidth=2)\n",
    "wavio.write(\"Resynthesis.wav\", audio_gen.numpy()[0], sample_rate, sampwidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
